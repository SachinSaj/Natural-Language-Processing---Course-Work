{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Extraction\n",
    "- One-hot Encoding\n",
    "- Bag of Words\n",
    "- Ngram\n",
    "- TF-IDF\n",
    "- Word2Vec"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### One-hot Encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from nltk.tokenize import word_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "ohe = OneHotEncoder(handle_unknown = 'ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['This', 'is', 'the', 'first', 'document', '.'], ['This', 'document', 'is', 'the', 'second', 'document', '.'], ['And', 'this', 'is', 'the', 'third', 'one', '.'], ['Is', 'this', 'the', 'first', 'document', '?']]\n"
     ]
    }
   ],
   "source": [
    "text = ['This is the first document.','This document is the second document.','And this is the third one.',\n",
    "        'Is this the first document?']\n",
    "words = [word_tokenize(sent) for sent in text]   # Important Step!!!\n",
    "print(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['.', '?', 'And', 'Is', 'This', 'document', 'first', 'is', 'one', 'second', 'the', 'third', 'this']\n"
     ]
    }
   ],
   "source": [
    "tokens = []\n",
    "for i in range(len(words)):\n",
    "    tokens = tokens + words[i]\n",
    "tokens = list(sorted(set(tokens)))            # only tokens (full tokens), set tokens(unique tokens)\n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['.'], ['?'], ['And'], ['Is'], ['This'], ['document'], ['first'], ['is'], ['one'], ['second'], ['the'], ['third'], ['this']]\n"
     ]
    }
   ],
   "source": [
    "vo = [[w] for w in tokens]\n",
    "print(vo)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.]])"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = ohe.fit_transform(vo).toarray() # one hot Encoding\n",
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([['.']], dtype=object)"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ohe.inverse_transform([[1,0,0,0,0,0,0,0,0,0,0,0,0]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ohe.transform([['sachin']]).toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.]])"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ohe.transform([['this']]).toarray()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bag of words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['This is the first document.',\n",
       " 'This document is the second document.',\n",
       " 'And this is the third one.',\n",
       " 'Is this the first document?']"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0 1 1 1 0 0 1 0 1]\n",
      " [0 2 0 1 0 1 1 0 1]\n",
      " [1 0 0 1 1 0 1 1 1]\n",
      " [0 1 1 1 0 0 1 0 1]]\n"
     ]
    }
   ],
   "source": [
    "# word level\n",
    "vectorizer = CountVectorizer()\n",
    "X = vectorizer.fit_transform(text).toarray()\n",
    "print(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['and', 'document', 'first', 'is', 'one', 'second', 'the', 'third', 'this']\n"
     ]
    }
   ],
   "source": [
    "vocab = vectorizer.get_feature_names()\n",
    "print(vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0 1 1 0 0 0 1 0 1]]\n"
     ]
    }
   ],
   "source": [
    "new_word = vectorizer.transform(['This can be the first document']).toarray()\n",
    "print(new_word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[4, 1, 0, 0, 1, 1, 2, 1, 2, 3, 1, 1, 1, 1, 3, 4, 1],\n",
       "       [5, 1, 0, 0, 3, 3, 4, 0, 2, 2, 2, 3, 3, 0, 3, 4, 2],\n",
       "       [5, 1, 0, 1, 0, 2, 2, 0, 3, 3, 0, 2, 1, 1, 2, 3, 0],\n",
       "       [4, 0, 1, 0, 1, 1, 2, 1, 2, 3, 1, 1, 1, 1, 3, 4, 1]], dtype=int64)"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Char Level\n",
    "vectorizer = CountVectorizer(analyzer = 'char') # to char lever ('a','b'etc)\n",
    "X = vectorizer.fit_transform(text).toarray()\n",
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[' ',\n",
       " '.',\n",
       " '?',\n",
       " 'a',\n",
       " 'c',\n",
       " 'd',\n",
       " 'e',\n",
       " 'f',\n",
       " 'h',\n",
       " 'i',\n",
       " 'm',\n",
       " 'n',\n",
       " 'o',\n",
       " 'r',\n",
       " 's',\n",
       " 't',\n",
       " 'u']"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab = vectorizer.get_feature_names()\n",
    "list(vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[5, 0, 0, 1, 2, 1, 2, 1, 1, 2, 2, 2, 1, 1, 2, 3, 1]], dtype=int64)"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_word = vectorizer.transform(['This can be my first document']).toarray()\n",
    "new_word"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### N-gram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0],\n",
       "       [0, 0, 2, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0],\n",
       "       [1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0],\n",
       "       [0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1]],\n",
       "      dtype=int64)"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Word level\n",
    "vectorizer = CountVectorizer(ngram_range = (1,2))\n",
    "X = vectorizer.fit_transform(text).toarray()\n",
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['and',\n",
       " 'and this',\n",
       " 'document',\n",
       " 'document is',\n",
       " 'first',\n",
       " 'first document',\n",
       " 'is',\n",
       " 'is the',\n",
       " 'is this',\n",
       " 'one',\n",
       " 'second',\n",
       " 'second document',\n",
       " 'the',\n",
       " 'the first',\n",
       " 'the second',\n",
       " 'the third',\n",
       " 'third',\n",
       " 'third one',\n",
       " 'this',\n",
       " 'this document',\n",
       " 'this is',\n",
       " 'this the']"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab = vectorizer.get_feature_names()\n",
    "vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[4, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 2, 1, 0, 0, 1,\n",
       "        1, 1, 2, 1, 1, 3, 1, 2, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 3, 2,\n",
       "        0, 1, 4, 1, 1, 0, 2, 1, 1],\n",
       "       [5, 2, 0, 1, 0, 1, 1, 1, 0, 0, 0, 3, 1, 2, 3, 1, 2, 4, 1, 0, 1, 2,\n",
       "        0, 0, 2, 1, 1, 2, 0, 2, 2, 2, 3, 1, 0, 2, 3, 2, 1, 0, 0, 0, 3, 2,\n",
       "        1, 0, 4, 1, 1, 0, 2, 2, 2],\n",
       "       [5, 0, 0, 1, 1, 0, 3, 1, 0, 1, 1, 0, 0, 0, 2, 2, 0, 2, 1, 1, 0, 0,\n",
       "        0, 0, 3, 1, 2, 3, 1, 2, 0, 0, 2, 1, 1, 0, 1, 0, 1, 1, 1, 0, 2, 2,\n",
       "        0, 0, 3, 0, 0, 0, 3, 0, 0],\n",
       "       [4, 1, 1, 0, 0, 0, 2, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 2, 1, 0, 0, 1,\n",
       "        1, 1, 2, 1, 1, 3, 1, 2, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 3, 2,\n",
       "        0, 1, 4, 1, 0, 1, 2, 1, 1]], dtype=int64)"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# char level\n",
    "vectorizer = CountVectorizer(ngram_range = (1,2), analyzer = 'char')\n",
    "X = vectorizer.fit_transform(text).toarray()\n",
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[' ',\n",
       " ' d',\n",
       " ' f',\n",
       " ' i',\n",
       " ' o',\n",
       " ' s',\n",
       " ' t',\n",
       " '.',\n",
       " '?',\n",
       " 'a',\n",
       " 'an',\n",
       " 'c',\n",
       " 'co',\n",
       " 'cu',\n",
       " 'd',\n",
       " 'd ',\n",
       " 'do',\n",
       " 'e',\n",
       " 'e ',\n",
       " 'e.',\n",
       " 'ec',\n",
       " 'en',\n",
       " 'f',\n",
       " 'fi',\n",
       " 'h',\n",
       " 'he',\n",
       " 'hi',\n",
       " 'i',\n",
       " 'ir',\n",
       " 'is',\n",
       " 'm',\n",
       " 'me',\n",
       " 'n',\n",
       " 'nd',\n",
       " 'ne',\n",
       " 'nt',\n",
       " 'o',\n",
       " 'oc',\n",
       " 'on',\n",
       " 'r',\n",
       " 'rd',\n",
       " 'rs',\n",
       " 's',\n",
       " 's ',\n",
       " 'se',\n",
       " 'st',\n",
       " 't',\n",
       " 't ',\n",
       " 't.',\n",
       " 't?',\n",
       " 'th',\n",
       " 'u',\n",
       " 'um']"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab = vectorizer.get_feature_names()\n",
    "vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.        , 0.46979139, 0.58028582, 0.38408524, 0.        ,\n",
       "        0.        , 0.38408524, 0.        , 0.38408524],\n",
       "       [0.        , 0.6876236 , 0.        , 0.28108867, 0.        ,\n",
       "        0.53864762, 0.28108867, 0.        , 0.28108867],\n",
       "       [0.51184851, 0.        , 0.        , 0.26710379, 0.51184851,\n",
       "        0.        , 0.26710379, 0.51184851, 0.26710379],\n",
       "       [0.        , 0.46979139, 0.58028582, 0.38408524, 0.        ,\n",
       "        0.        , 0.38408524, 0.        , 0.38408524]])"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# TF-IDF\n",
    "# Wordlevel\n",
    "vectorizer = TfidfVectorizer()\n",
    "X = vectorizer.fit_transform(text).toarray()\n",
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['and', 'document', 'first', 'is', 'one', 'second', 'the', 'third', 'this']"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab = vectorizer.get_feature_names()\n",
    "vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.        , 0.55953044, 0.69113141, 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.4574528 ]])"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_word = vectorizer.transform(['This can be my first document']).toarray()\n",
    "new_word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.47550697, 0.14540332, 0.        , 0.        , 0.14540332,\n",
       "        0.11887674, 0.23775349, 0.17960203, 0.23775349, 0.35663023,\n",
       "        0.14540332, 0.11887674, 0.11887674, 0.14540332, 0.35663023,\n",
       "        0.47550697, 0.14540332],\n",
       "       [0.44206359, 0.10814145, 0.        , 0.        , 0.32442434,\n",
       "        0.26523816, 0.35365088, 0.        , 0.17682544, 0.17682544,\n",
       "        0.21628289, 0.26523816, 0.26523816, 0.        , 0.26523816,\n",
       "        0.35365088, 0.21628289],\n",
       "       [0.57481012, 0.14061506, 0.        , 0.22030066, 0.        ,\n",
       "        0.22992405, 0.22992405, 0.        , 0.34488607, 0.34488607,\n",
       "        0.        , 0.22992405, 0.11496202, 0.14061506, 0.22992405,\n",
       "        0.34488607, 0.        ],\n",
       "       [0.46836004, 0.        , 0.2243785 , 0.        , 0.14321789,\n",
       "        0.11709001, 0.23418002, 0.17690259, 0.23418002, 0.35127003,\n",
       "        0.14321789, 0.11709001, 0.11709001, 0.14321789, 0.35127003,\n",
       "        0.46836004, 0.14321789]])"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Char level\n",
    "vectorizer = TfidfVectorizer(analyzer = 'char')\n",
    "X = vectorizer.fit_transform(text).toarray()\n",
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[' ',\n",
       " '.',\n",
       " '?',\n",
       " 'a',\n",
       " 'c',\n",
       " 'd',\n",
       " 'e',\n",
       " 'f',\n",
       " 'h',\n",
       " 'i',\n",
       " 'm',\n",
       " 'n',\n",
       " 'o',\n",
       " 'r',\n",
       " 's',\n",
       " 't',\n",
       " 'u']"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab = vectorizer.get_feature_names()\n",
    "vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.18466915, 0.22810314, 0.18466915, 0.        , 0.        ,\n",
       "        0.15097913, 0.        , 0.        , 0.18466915, 0.        ,\n",
       "        0.18466915, 0.15097913, 0.        , 0.        , 0.18466915,\n",
       "        0.22810314, 0.15097913, 0.15097913, 0.18466915, 0.30195826,\n",
       "        0.18466915, 0.        , 0.        , 0.18466915, 0.18466915,\n",
       "        0.        , 0.        , 0.22810314, 0.30195826, 0.        ,\n",
       "        0.22810314, 0.18466915, 0.22810314, 0.        , 0.30195826,\n",
       "        0.18466915],\n",
       "       [0.25688446, 0.        , 0.12844223, 0.        , 0.20122957,\n",
       "        0.10500994, 0.        , 0.20122957, 0.25688446, 0.1586517 ,\n",
       "        0.25688446, 0.10500994, 0.        , 0.20122957, 0.25688446,\n",
       "        0.        , 0.10500994, 0.10500994, 0.        , 0.21001987,\n",
       "        0.25688446, 0.1586517 , 0.        , 0.25688446, 0.25688446,\n",
       "        0.1586517 , 0.        , 0.        , 0.21001987, 0.20122957,\n",
       "        0.        , 0.12844223, 0.1586517 , 0.        , 0.21001987,\n",
       "        0.25688446],\n",
       "       [0.        , 0.        , 0.14937663, 0.23402735, 0.        ,\n",
       "        0.36637554, 0.23402735, 0.        , 0.        , 0.36901971,\n",
       "        0.        , 0.12212518, 0.23402735, 0.        , 0.        ,\n",
       "        0.        , 0.12212518, 0.24425036, 0.14937663, 0.24425036,\n",
       "        0.        , 0.18450985, 0.23402735, 0.        , 0.        ,\n",
       "        0.18450985, 0.23402735, 0.        , 0.24425036, 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.36637554,\n",
       "        0.        ],\n",
       "       [0.1788647 , 0.22093349, 0.        , 0.        , 0.        ,\n",
       "        0.29246722, 0.        , 0.        , 0.1788647 , 0.        ,\n",
       "        0.1788647 , 0.14623361, 0.        , 0.        , 0.1788647 ,\n",
       "        0.22093349, 0.14623361, 0.14623361, 0.1788647 , 0.29246722,\n",
       "        0.1788647 , 0.        , 0.        , 0.1788647 , 0.1788647 ,\n",
       "        0.        , 0.        , 0.22093349, 0.29246722, 0.        ,\n",
       "        0.22093349, 0.1788647 , 0.        , 0.28022611, 0.29246722,\n",
       "        0.1788647 ]])"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# char level\n",
    "vectorizer = TfidfVectorizer(analyzer = 'char', ngram_range = (2,2))\n",
    "X = vectorizer.fit_transform(text).toarray()\n",
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Char level\n",
    "vectorizer = TfidfVectorizer(ngram_range=(2,2))\n",
    "X = vectorizer.fit_transform(text).toarray()\n",
    "X"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Word2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\gensim\\utils.py:1197: UserWarning: detected Windows; aliasing chunkize to chunkize_serial\n",
      "  warnings.warn(\"detected Windows; aliasing chunkize to chunkize_serial\")\n"
     ]
    }
   ],
   "source": [
    "from gensim.models import Word2Vec\n",
    "from nltk.tokenize import word_tokenize\n",
    "from gensim.models import Phrases\n",
    "from gensim.models.doc2vec import Doc2Vec, TaggedDocument\n",
    "from gensim.models import FastText"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = ['This is the first document.','This is the second second document.',\n",
    "        'And the third one.',\n",
    "        'Is this the first document?']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['This', 'is', 'the', 'first', 'document', '.'], ['This', 'is', 'the', 'second', 'second', 'document', '.'], ['And', 'the', 'third', 'one', '.'], ['Is', 'this', 'the', 'first', 'document', '?']]\n"
     ]
    }
   ],
   "source": [
    "tokens = [word_tokenize(text) for text in data]\n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Word2Vec(tokens, size=5,window=5,min_count=1,workers=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'This': <gensim.models.keyedvectors.Vocab at 0x18031ac8080>,\n",
       " 'is': <gensim.models.keyedvectors.Vocab at 0x18031ac80f0>,\n",
       " 'the': <gensim.models.keyedvectors.Vocab at 0x18031ac80b8>,\n",
       " 'first': <gensim.models.keyedvectors.Vocab at 0x18031ac8128>,\n",
       " 'document': <gensim.models.keyedvectors.Vocab at 0x18031ac8160>,\n",
       " '.': <gensim.models.keyedvectors.Vocab at 0x18031ac8198>,\n",
       " 'second': <gensim.models.keyedvectors.Vocab at 0x18031ac81d0>,\n",
       " 'And': <gensim.models.keyedvectors.Vocab at 0x18031ac8208>,\n",
       " 'third': <gensim.models.keyedvectors.Vocab at 0x18031ac8240>,\n",
       " 'one': <gensim.models.keyedvectors.Vocab at 0x18031ac8278>,\n",
       " 'Is': <gensim.models.keyedvectors.Vocab at 0x18031ac82b0>,\n",
       " 'this': <gensim.models.keyedvectors.Vocab at 0x18031ac82e8>,\n",
       " '?': <gensim.models.keyedvectors.Vocab at 0x18031ac8320>}"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.wv.vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-0.05589332,  0.02646712,  0.01573865, -0.09324984, -0.00161548],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.wv['This']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:1: DeprecationWarning: Call to deprecated `__getitem__` (Method will be removed in 4.0.0, use self.wv.__getitem__() instead).\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([-0.05589332,  0.02646712,  0.01573865, -0.09324984, -0.00161548],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model['This']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\smart_open\\smart_open_lib.py:398: UserWarning: This function is deprecated, use smart_open.open instead. See the migration notes for details: https://github.com/RaRe-Technologies/smart_open/blob/master/README.rst#migrating-to-the-new-open-function\n",
      "  'See the migration notes for details: %s' % _MIGRATION_NOTES_URL\n"
     ]
    }
   ],
   "source": [
    "model.save('Word2Vec.model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Word2Vec.load('Word2Vec.model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('This', 1.0),\n",
       " ('this', 0.5043507218360901),\n",
       " ('one', 0.4586126506328583),\n",
       " ('Is', 0.26038751006126404),\n",
       " ('?', 0.12596315145492554),\n",
       " ('the', -0.04200264811515808),\n",
       " ('document', -0.1248597502708435),\n",
       " ('first', -0.1417647898197174),\n",
       " ('third', -0.29639196395874023),\n",
       " ('is', -0.37558913230895996)]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.wv.similar_by_vector(model.wv['This'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('this', 0.5043507218360901),\n",
       " ('one', 0.4586126208305359),\n",
       " ('Is', 0.26038751006126404),\n",
       " ('?', 0.12596315145492554),\n",
       " ('the', -0.04200264811515808),\n",
       " ('document', -0.1248597651720047),\n",
       " ('first', -0.1417647749185562),\n",
       " ('third', -0.29639193415641785),\n",
       " ('is', -0.37558913230895996),\n",
       " ('And', -0.41193658113479614)]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.wv.similar_by_word('This')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "defaultdict(<class 'int'>, {b'This': 2, b'is': 2, b'This_is': 2, b'the': 4, b'is_the': 2, b'first': 2, b'the_first': 2, b'document': 3, b'first_document': 2, b'.': 3, b'document_.': 2, b'second': 2, b'the_second': 1, b'second_second': 1, b'second_document': 1, b'And': 1, b'And_the': 1, b'third': 1, b'the_third': 1, b'one': 1, b'third_one': 1, b'one_.': 1, b'Is': 1, b'this': 1, b'Is_this': 1, b'this_the': 1, b'?': 1, b'document_?': 1})\n"
     ]
    }
   ],
   "source": [
    "bigram_transformer = Phrases(tokens)\n",
    "print(bigram_transformer.vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\gensim\\models\\phrases.py:494: UserWarning: For a faster implementation, use the gensim.models.phrases.Phraser class\n",
      "  warnings.warn(\"For a faster implementation, use the gensim.models.phrases.Phraser class\")\n"
     ]
    }
   ],
   "source": [
    "model = Word2Vec(bigram_transformer[tokens], min_count=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "documents = [TaggedDocument(doc, [i]) for i, doc in enumerate(tokens)]\n",
    "model = Doc2Vec(documents, vector_size=5, window=2, min_count=1, workers=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.02028376,  0.07223367, -0.0845509 ,  0.04373696, -0.09653375],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.infer_vector([\"this is\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.04542537, -0.06584745,  0.00271636, -0.00229087,  0.07842097],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.infer_vector([\"this\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'This': <gensim.models.keyedvectors.Vocab at 0x18031b46828>,\n",
       " 'is': <gensim.models.keyedvectors.Vocab at 0x18031b46860>,\n",
       " 'the': <gensim.models.keyedvectors.Vocab at 0x18031b46898>,\n",
       " 'first': <gensim.models.keyedvectors.Vocab at 0x18031b468d0>,\n",
       " 'document': <gensim.models.keyedvectors.Vocab at 0x18031b46908>,\n",
       " '.': <gensim.models.keyedvectors.Vocab at 0x18031b46940>,\n",
       " 'second': <gensim.models.keyedvectors.Vocab at 0x18031b46978>,\n",
       " 'And': <gensim.models.keyedvectors.Vocab at 0x18031b469b0>,\n",
       " 'third': <gensim.models.keyedvectors.Vocab at 0x18031b469e8>,\n",
       " 'one': <gensim.models.keyedvectors.Vocab at 0x18031b46a20>,\n",
       " 'Is': <gensim.models.keyedvectors.Vocab at 0x18031b46a58>,\n",
       " 'this': <gensim.models.keyedvectors.Vocab at 0x18031b46a90>,\n",
       " '?': <gensim.models.keyedvectors.Vocab at 0x18031b46ac8>}"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.wv.vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = FastText(size=4, window=3, min_count=1) \n",
    "model.build_vocab(sentences=tokens)\n",
    "model.train(sentences=tokens, total_examples=len(tokens), epochs=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-0.02406601,  0.02835182,  0.01417441,  0.0065731 ], dtype=float32)"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.wv[\"This\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
